{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60b3e05",
   "metadata": {},
   "source": [
    "# BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ee9264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'Content-Type': 'text/html; charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Date': 'Sun, 22 Dec 2024 17:57:26 GMT', 'Set-Cookie': 'AWSALBTG=kPiJ+Vqs9pYUGUEeYaiGdAEfXZe6azIDpYUAhiqJbp6Xcm3ggnsWwhHwp1HPrCQ54wyTzTdThMu7989OWmGG7khSWvXb4lbSKx8aBagLdTWRbY+voS0W7zIi9VbaRW+Hx3mo9hGI2jXaiTcxF2Snw6MrhEfXw1ZuvfLPbbph+Zd+nC3xCBw=; Expires=Sun, 29 Dec 2024 17:57:26 GMT; Path=/, AWSALBTGCORS=kPiJ+Vqs9pYUGUEeYaiGdAEfXZe6azIDpYUAhiqJbp6Xcm3ggnsWwhHwp1HPrCQ54wyTzTdThMu7989OWmGG7khSWvXb4lbSKx8aBagLdTWRbY+voS0W7zIi9VbaRW+Hx3mo9hGI2jXaiTcxF2Snw6MrhEfXw1ZuvfLPbbph+Zd+nC3xCBw=; Expires=Sun, 29 Dec 2024 17:57:26 GMT; Path=/; SameSite=None; Secure, STATSESSID=g5roqo8vn89pki64jf57a5enha; expires=Fri, 20-Jun-2025 17:57:26 GMT; Max-Age=15552000; path=/; domain=statista.com; secure; HttpOnly, statistaContentView=1549506576768530631ed79061359865; expires=Tue, 22-Dec-2026 17:57:26 GMT; Max-Age=63072000; path=/; domain=statista.com; secure; httponly', 'Server': 'nginx', 'Cache-Control': 'max-age=0, must-revalidate, private', 'Expires': 'Sun, 22 Dec 2024 17:57:26 GMT', 'x-cache-status': 'BYPASS', 'x-frame-options': 'DENY', 'x-content-type-options': 'nosniff', 'x-xss-protection': '1; mode=block', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'content-security-policy': \"default-src 'self' https: ; script-src 'self' 'unsafe-inline' 'unsafe-eval' https: ; img-src 'self' data: https: ; font-src 'self' data: https://cdn.statcdn.com/ ; style-src 'self' 'unsafe-inline' https://cdn.statcdn.com/ https://content.pendo.statista.com/ ; object-src 'self' https://cdn.statcdn.com/ ; frame-src 'self' https: ; frame-ancestors 'none' ; connect-src 'self' https: wss://ws.hotjar.com\", 'x-proxy-flow': 'route-to-legacy', 'Vary': 'Accept-Encoding', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 83e5693f5fb682f375eb4d52fbd47ca8.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'BCN50-P2', 'X-Amz-Cf-Id': 'S5i4OPZOjkJb9VnUEj6qG_FIwLw03ryipLv-KvEy8pVyJXlSVDuEfA==', 'Referrer-Policy': 'origin-when-cross-origin', 'Cross-Origin-Embedder-PolicyStatement': 'credentialless', 'Cross-Origin-Opener-PolicyStatement': 'same-origin', 'Permissions-Policy': 'geolocation=(self)'}\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m\n\u001b[0;32m     32\u001b[0m src \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Ahora que tenemos el código fuente de la página almacenado,\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# utilizaremos el módulo BeautifulSoup para analizar y procesar\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# el código fuente. Para hacerlo, creamos un objeto BeautifulSoup\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# basado en la variable de fuente que creamos arriba:\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Ahora que el código fuente ha sido procesado por BeautifulSoup,\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# podemos acceder a información específica directamente desde él.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Por ejemplo, digamos que queremos ver una lista de todos los\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# enlaces de la página:\u001b[39;00m\n\u001b[0;32m     44\u001b[0m links \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\bs4\\__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# Asegúrate de tener instalados tanto beautifulsoup como requests:\n",
    "#   pip install beautifulsoup4\n",
    "#   pip install requests\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Usando el módulo requests, usamos la función \"get\"\n",
    "# proporcionada para acceder a la página web indicada\n",
    "# como argumento en esta función:\n",
    "result = requests.get(\"https://es.statista.com/estadisticas/622465/puntos-de-recarga-de-vehiculos-electricos-por-comunidad-autonoma-espana/\")\n",
    "\n",
    "# Para asegurarnos de que el sitio web es accesible,\n",
    "# podemos verificar que obtengamos una respuesta 200 OK\n",
    "# que indica que la página está presente:\n",
    "print(result.status_code)\n",
    "\n",
    "# Para otros posibles códigos de estado que podrías encontrar,\n",
    "# consulta la siguiente página de Wikipedia:\n",
    "# https://es.wikipedia.org/wiki/C%C3%B3digo_de_estado_HTTP\n",
    "\n",
    "# También podemos revisar el encabezado HTTP del sitio web para\n",
    "# verificar que realmente hemos accedido a la página correcta:\n",
    "print(result.headers)\n",
    "\n",
    "# Para más información sobre los encabezados HTTP y la información\n",
    "# que se puede obtener de ellos, puedes consultar:\n",
    "# https://es.wikipedia.org/wiki/Encabezado_HTTP\n",
    "\n",
    "# Ahora, almacenamos el contenido de la página del sitio web\n",
    "# que obtuvimos con requests en una variable:\n",
    "src = result.content\n",
    "\n",
    "# Ahora que tenemos el código fuente de la página almacenado,\n",
    "# utilizaremos el módulo BeautifulSoup para analizar y procesar\n",
    "# el código fuente. Para hacerlo, creamos un objeto BeautifulSoup\n",
    "# basado en la variable de fuente que creamos arriba:\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "# Ahora que el código fuente ha sido procesado por BeautifulSoup,\n",
    "# podemos acceder a información específica directamente desde él.\n",
    "# Por ejemplo, digamos que queremos ver una lista de todos los\n",
    "# enlaces de la página:\n",
    "links = soup.find_all(\"a\")\n",
    "print(links)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Tal vez solo queramos extraer el enlace que contenga el texto\n",
    "# \"About\" en la página en lugar de todos los enlaces. Podemos usar\n",
    "# la función incorporada \"text\" para acceder al contenido de texto\n",
    "# entre las etiquetas <a> </a>.\n",
    "for link in links:\n",
    "    if \"About\" in link.text:\n",
    "        link\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd39360",
   "metadata": {},
   "source": [
    "# SELENIUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eaa7387",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'deprecated' from 'typing_extensions' (C:\\Users\\ruben\\anaconda3\\lib\\site-packages\\typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Configuración de Selenium\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\__init__.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriver \u001b[38;5;28;01mas\u001b[39;00m ChromiumEdge  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriver \u001b[38;5;28;01mas\u001b[39;00m Edge  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfirefox\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfirefox_profile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FirefoxProfile  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfirefox\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Options \u001b[38;5;28;01mas\u001b[39;00m FirefoxOptions  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfirefox\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Service \u001b[38;5;28;01mas\u001b[39;00m FirefoxService  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\firefox\\firefox_profile.py:31\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minidom\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriverException\n\u001b[0;32m     35\u001b[0m WEBDRIVER_PREFERENCES \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwebdriver_prefs.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'deprecated' from 'typing_extensions' (C:\\Users\\ruben\\anaconda3\\lib\\site-packages\\typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# Configuración de Selenium\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Cargar la página\n",
    "url = \"https://es.statista.com/estadisticas/730785/vehiculos-electricos-matriculados-por-comunidad-autonoma-espana/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Esperar a que se cargue el contenido dinámico\n",
    "time.sleep(5)\n",
    "\n",
    "# Buscar la tabla o los datos de vehículos eléctricos\n",
    "try:\n",
    "    # Localiza elementos por etiquetas o clases específicas\n",
    "    table = driver.find_element(By.TAG_NAME, \"table\")  # Cambiar si es necesario\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "    \n",
    "    for row in rows:\n",
    "        columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        data = [col.text.strip() for col in columns]\n",
    "        print(data)\n",
    "except Exception as e:\n",
    "    print(f\"Error al extraer datos: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae633b",
   "metadata": {},
   "source": [
    "# API GOOGLE MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a4afe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall selenium typing_extensions\n",
    "!pip install selenium typing_extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40b013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 2 scraped. Waiting 5 seconds before next request...\n",
      "Data successfully saved to OneDrive - CENTRO EDUCATIVO FASTA MADRE SACRAMENTOEscritorioCIENCIA DE DATOS2ºATDProyecto_ATD\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "import requests\n",
    "from lxml import etree\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "import time\n",
    "\n",
    "def scrape_statista_pages(start_page, end_page, delay=5):\n",
    "    \"\"\"\n",
    "    Scrape Statista pages from start_page to end_page.\n",
    "\n",
    "    Parameters:\n",
    "    - start_page (int): The first page number to scrape.\n",
    "    - end_page (int): The last page number to scrape.\n",
    "    - delay (int): Delay in seconds between requests to avoid being blocked.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 '\n",
    "                      '(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "    }\n",
    "\n",
    "    all_data = {\n",
    "        'Topics': [],\n",
    "        'Report Title': [],\n",
    "        'Description': [],\n",
    "        'Published Date': [],\n",
    "        'Image Source': [],\n",
    "    }\n",
    "\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        url = f\"https://www.statista.com/chartoftheday/ALL/p/{page_number}/\"\n",
    "        page_text = requests.get(url=url, headers=headers).text\n",
    "\n",
    "        tree = etree.HTML(page_text)\n",
    "        topics = tree.xpath('//figure/figcaption/div/h3/text()')\n",
    "        reports_titles = tree.xpath('//figure/img/@title')\n",
    "        descriptions = tree.xpath('//figure/figcaption/text()')\n",
    "        published_dates = tree.xpath('//figure/figcaption/div/time/text()')\n",
    "        img_src = tree.xpath('//figure/img/@src')\n",
    "\n",
    "        all_data['Topics'].extend(topics)\n",
    "        all_data['Report Title'].extend(reports_titles)\n",
    "        all_data['Description'].extend(descriptions)\n",
    "        all_data['Published Date'].extend(published_dates)\n",
    "        all_data['Image Source'].extend(img_src)\n",
    "\n",
    "        print(f\"Page {page_number} scraped. Waiting {delay} seconds before next request...\")\n",
    "        time.sleep(delay)  # Delay to avoid being blocked\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def upload_data_to_excel_openpyxl(data, excel_filename):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "\n",
    "    max_length = {}\n",
    "    for col, header in enumerate(data.keys(), start=1):\n",
    "        ws.cell(row=1, column=col, value=header)\n",
    "        max_length[col] = len(header)\n",
    "\n",
    "    for col_index, (key, values) in enumerate(data.items(), start=1):\n",
    "        for row_index, value in enumerate(values, start=2):\n",
    "            ws.cell(row=row_index, column=col_index, value=value)\n",
    "            max_length[col_index] = max(max_length[col_index], len(str(value)))\n",
    "\n",
    "    for col, length in max_length.items():\n",
    "        ws.column_dimensions[get_column_letter(col)].width = length\n",
    "\n",
    "    wb.save(filename=excel_filename)\n",
    "    print(f\"Data successfully saved to {excel_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape pages 2 through 137 with a delay of 5 seconds between each request\n",
    "    scraped_data = scrape_statista_pages(2, 2, delay=5)\n",
    "\n",
    "    # Save scraped data to Excel\n",
    "    excel_filename = 'OneDrive - CENTRO EDUCATIVO FASTA MADRE SACRAMENTOEscritorioCIENCIA DE DATOS2ºATDProyecto_ATD'\n",
    "    upload_data_to_excel_openpyxl(scraped_data, excel_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
